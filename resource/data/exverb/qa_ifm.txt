We use extractive multi-document summarization techniques to perform complex question answering and formulate it as a reinforcement learning problem. 

Given a set of complex questions, a list of relevant documents per question, and the corresponding human generated summaries as training data, the reinforcement learning module iteratively learns a number of feature weights in order to facilitate the automatic generation of summaries i.e. answers to previously unseen complex questions. 
A reward function is used to measure the similarities between the candidate (machine generated) summary sentences and the abstract summaries. 
In the training stage, the learner iteratively selects the important document sentences to be included in the candidate summary, analyzes the reward function and updated the related feature weights accordingly. The final weights are used to generate summaries as answers to unseen complex questions in the testing stage. Evaluation results show the effectiveness of our system. We also incorporate user interaction into the reinforcement learner to guide the candidate summary sentence selection process. Experiments reveal the positive impact of the user interaction component on the reinforcement learning framework.

1. Introduction
The increasing demand for access to different types of information available online have led researchers to a renewed interest in a broad range of Information Retrieval (IR) related areas such as question answering, topic detection and tracking, summarization, multimedia retrieval, chemical and biological informatics, text structuring, and text mining. The existing document retrieval systems cannot satisfy the end-users’ information need to have more direct access into relevant documents. Question Answering (QA) systems can address this challenge effectively (Strzalkowski & Harabagiu, 2006). The human variant of the QA systems were used effectively over the years. One such system is the popular QA system in Korea, the Korean Naver’s Knowledge iN search,1 which allows users to ask almost any question and get answers from other users (Chali, Joty, & Hasan, 2009). Another widely known QA service is Yahoo! Answers2 which is a community-driven knowledge market website launched by Yahoo!. As of December 2009, Yahoo! Answers had 200 million users worldwide and more than 1 billion answers.3 Furthermore, Google launched a QA system4 in April 2002 that was based on paid editors. However, the system was closed in December 2006. The main limitation of these QA systems is that they rely on human expertise to help provide the answers. Our goal is to automate this process such that computers can do the same as professional information analysts. This research is a small step towards such an ambitious goal.

QA research can handle different types of questions: fact, list, definition, how, why, etc. Some questions, which we call simple questions, are easier to answer. For example, the question: “Who is the prime minister of Canada?” asks for a person’s name. This type of question (i.e. factoid) requires small snippets of text as the answer. On the other hand, complex questions often require multiple types of information. For example, the question: “How was Japan affected by the earthquake?” suggests that the inquirer is looking for information in the context of a wider perspective. Multi-document summarization techniques can be applied to treat these questions successfully ( Chali, Hasan and Joty, 2011, Chali, Joty, et al., 2009 and Harabagiu et al., 2006).

Multi-document summarization can be used to describe the information of a document collection in a concise manner (Wan, Yang, & Xiao, 2007a). Some web-based systems are already utilizing the potential of this technology. For example, Google News5 and the Newsblaster6 system automatically collect, cluster, categorize, and summarize news from several sites on the web, and help users find news of their interest. In the last decade, complex questions have received much attention from both the Question Answering (QA) and Multi-document Summarization (MDS) communities (Carbonell, Harman, Hovy, Maiorano, & Prange, et al., 2000). Typically, complex QA evaluation systems including the 2004 AQUAINT Relationship QA Pilot,7 the 2005 Text Retrieval Conference (TREC) Relationship QA Task,8 and the TREC definition9 return unstructured lists of candidate answers in response to a complex question. The MDS evaluations (including the 2005, 2006 and 2007 Document Understanding Conference (DUC10) task systems with returning paragraph-length answers to complex questions that are responsive, relevant, and coherent. Complex question answering in the form of a query-focused multi-document summarization task is useful in the domain of document management and search systems. For example, it can provide personalized news services for different users according to the users’ unique information need (Wan et al., 2009). Moreover, users can obtain the news about a single event from different sources in the form of a summary containing multiple perspectives at the same time.

This paper is concerned with automatic answering of complex questions. We define the complex questions as the kind of questions whose answers need to be obtained from pieces of information scattered in different documents. Our experiments and evaluations were mainly influenced by the specific scenario proposed by the DUC (2005–2007) tasks. In fact, DUC proposes a query-focused summarization task whose features have allowed us to simulate our experiments with complex question answering. Hence, the considered complex questions are the type of questions that request information such as an elaboration about a topic, description about an event or entity, illustration about an opinion, and definition or discussion about an aspect or term or procedure. We use an extractive11 multi-document summarization approach to perform the complex question answering task.

Effective complex question answering can aid to the improvement of the search systems. When a user searches for some information, the traditional search engines usually offer a listing of sources through which the user has to continue navigating until the desired information need is satisfied. Moreover, the available search systems lack a way of measuring the level of user satisfaction which could have been used to enhance the search policy in real time. User satisfaction can be observed effectively by monitoring user actions (e.g., copy-pasting, printing, saving, emailing) after the search results are presented. A user study can reveal the relationship between user satisfaction and retrieval effectiveness (Al-Maskari, Sanderson, & Clough, 2007). Zaragoza, Cambazoglu, and Baeza-Yates (2010) performed a quantitative analysis about what fraction of the web search queries (posed to the current search engines) can lead to satisfactory results. Computation of user satisfaction, as well as improvement to the search policy, is a difficult task to perform in real time. This motivates us to propose a reinforcement learning formulation to the complex question answering task so that the system can learn from user interaction to improve its accuracy according to user’s information need.

Formally, the complex question answering problem can be mapped to a reinforcement learning framework as follows: given a set of complex questions, a collection of relevant documents12 per question, and the corresponding human-generated summaries (i.e. answers to the questions), a reinforcement learning model can be trained to extract the most important sentences to form summaries (Chali, Hasan & Imam, 2011). Our main motivation behind the proposal of the reinforcement learning formulation is in fact to enable learning from human interaction in real time as we believe that the incorporation of user feedback into the learning process can lead to a robust system that produces more accurate summaries to increase the level of user satisfaction. However, for simplicity during the learning stage, we assume that initially there is no actual user interaction provided to the system rather the importance of a candidate document sentence can be verified by measuring its similarity with the given human-made abstract summary sentences using a reward function. This assumption relies on the intuition that the users are fully satisfied with the abstract summaries. The original document sentences that are mostly similar to the abstract summary sentences are assigned good reward values. In reinforcement learning, the learner is not aware of which actions (sentence selection in our case) to take, rather it must discover which actions deliver the most reward by trying them ( Sutton & Barto, 1998).

Real-time user interaction can help QA systems evolve by improving their policy automatically as time passes. Toward this end, we treat the complex question answering task as an interactive problem. Supervised learning techniques are alone not adequate for learning from interaction (Sutton & Barto, 1998). These techniques require a huge amount of human-annotated training data and it is often impossible to collect training examples of all desired kinds in which the agent has to act. Instead, a reinforcement learning approach can be used to sense the state of the environment and take suitable actions that affect the state. We assume that a small amount of supervision is provided in the form of a reward function that defines the quality of the executed actions. In the training stage, the reinforcement learner repeatedly defines action sequences, performs the actions, and observes the resulting reward. The learner’s goal is to estimate a policy that maximizes the expected future reward (Branavan, Chen, Zettlemoyer, & Barzilay, 2009).

In this paper, we present a reinforcement learning framework for answering complex questions. As noted before, we simplify our formulation by assuming no real time user interaction by considering that the human generated abstract summaries are the gold-standard and the users (if they were involved) are satisfied with them. The proposed system tries to produce automatic summaries that are as close as the abstract summaries. The relationship between these two types of summaries is learned and the final weights are used to output the machine generated summaries for the unseen data. We employ a modified linear, gradient-descent version of Watkins’ Q(λ) algorithm (Sutton & Barto, 1998) to estimate the parameters of our model. Experiments on the DUC benchmark datasets demonstrate the effectiveness of the reinforcement learning approach. We also extend this work by proposing a model that incorporates user interaction into the reinforcement learner to guide the candidate summary sentence selection process. Evaluation results indicate that the user interaction component further improves the performance of the reinforcement learning framework (Chali, Hasan, & Imam, 2012). The rest of the paper includes related work, our reinforcement learning formulation, feature space, user interaction modeling, experiments with results, and finally, conclusion with some future directions.

For the future , we are considering the use of our HPSG parser on PSTFS for a speech recognition system , a Natural Language Interface or Speech Machine Translation applications .

Broad-coverage relation extraction either requires expensive supervised training data, or suffers from drawbacks inherent to distant supervision. 
We present an approach for providing partial supervision to a distantly supervised relation extractor using a small number of carefully selected examples. 
We compare against established active learning criteria and propose a novel criterion to sample examples which are both uncertain and representative. 
In this way, we combine the benefits of fine-grained supervision for diffi- cult examples with the coverage of a large distantly supervised corpus. 
Our approach gives a substantial increase of 3.9% endto-end F1 on the 2013 KBP Slot Filling evaluation, yielding a net F1 of 37.7%. 